\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{chomsky2004estructuras}
\citation{orduna2011estudio}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Marco teórico}{2}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:chap2}{{2}{2}{Marco teórico}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Modelos del lenguaje}{2}{section.2.1}\protected@file@percent }
\newlabel{sec:sec21}{{2.1}{2}{Modelos del lenguaje}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Gramática Generativa de Chomsky}{2}{subsection.2.1.1}\protected@file@percent }
\newlabel{subsec:sec211}{{2.1.1}{2}{Gramática Generativa de Chomsky}{subsection.2.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{¿Por qué no debe usarse un modelo estadístico?}{2}{subsubsection*.8}\protected@file@percent }
\newlabel{subsubsec:sec2111}{{2.1.1}{2}{¿Por qué no debe usarse un modelo estadístico?}{subsubsection*.8}{}}
\citation{cohen1991introduction}
\citation{allerton1979essentials}
\citation{chomsky2004estructuras}
\@writefile{toc}{\contentsline {subsubsection}{Gramáticas de Contexto Libre}{3}{subsubsection*.9}\protected@file@percent }
\newlabel{subsubsec:sec2112}{{2.1.1}{3}{Gramáticas de Contexto Libre}{subsubsection*.9}{}}
\citation{chomsky2004estructuras}
\citation{chomsky1956three}
\citation{allerton1979essentials}
\citation{chomsky1956three}
\@writefile{toc}{\contentsline {subsubsection}{Transformaciones}{4}{subsubsection*.10}\protected@file@percent }
\citation{chomsky1997problemas}
\@writefile{toc}{\contentsline {subsubsection}{Morfofonémica}{5}{subsubsection*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Estructura distributiva de Harris}{5}{subsection.2.1.2}\protected@file@percent }
\citation{harris1954distributional}
\citation{sahlgren2008distributional}
\citation{harris1954distributional}
\citation{otero1975terminologia}
\citation{brants2007large}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Modelos computacionales con base en la hipótesis distributiva}{6}{section.2.2}\protected@file@percent }
\newlabel{sec:sec22}{{2.2}{6}{Modelos computacionales con base en la hipótesis distributiva}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}N-gramas}{6}{subsection.2.2.1}\protected@file@percent }
\citation{norvig209natural}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Distribución de la palabra oculista. En rojo se muestran los términos concurrentes únicamente con la palabra médico, de verde los términos concurrentes exclusivamente con la palabra abogado y en azul los términos concurrentes con ambas palabras. Gráfico de elaboración propia.\relax }}{7}{figure.caption.12}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:example_structure}{{2.1}{7}{Distribución de la palabra oculista. En rojo se muestran los términos concurrentes únicamente con la palabra médico, de verde los términos concurrentes exclusivamente con la palabra abogado y en azul los términos concurrentes con ambas palabras. Gráfico de elaboración propia.\relax }{figure.caption.12}{}}
\newlabel{eq:ngrams}{{2.1}{7}{N-gramas}{equation.2.2.1}{}}
\citation{mikolov2013efficient,mikolov2013distributed}
\citation{bruce2020practical}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Diagrama de una cadena de longitud $L+1$. Donde se muestra que la palabra $w_{L+1}$ depende de todas las palabras anteriores. Gráfico de elaboración propia. \relax }}{8}{figure.caption.13}\protected@file@percent }
\newlabel{fig:ngramfull}{{2.2}{8}{Diagrama de una cadena de longitud $L+1$. Donde se muestra que la palabra $w_{L+1}$ depende de todas las palabras anteriores. Gráfico de elaboración propia. \relax }{figure.caption.13}{}}
\newlabel{eq:ngramsok}{{2.3}{8}{N-gramas}{equation.2.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Diagrama de una cadena de longitud $L+1$. Donde se muestra que la palabra $w_{L+1}$ depende solo de un número limitado de palabras (en verde). Gráfico de elaboración propia. \relax }}{8}{figure.caption.14}\protected@file@percent }
\newlabel{fig:ngram}{{2.3}{8}{Diagrama de una cadena de longitud $L+1$. Donde se muestra que la palabra $w_{L+1}$ depende solo de un número limitado de palabras (en verde). Gráfico de elaboración propia. \relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Modelos CBOW y Skip-Gram}{8}{subsection.2.2.2}\protected@file@percent }
\citation{rong2014word2vec}
\newlabel{fig:cbow}{{2.4a}{9}{Diagrama de la arquitectura del modelo CBOW. Gráfico de elaboración propia. \relax }{figure.caption.15}{}}
\newlabel{sub@fig:cbow}{{a}{9}{Diagrama de la arquitectura del modelo CBOW. Gráfico de elaboración propia. \relax }{figure.caption.15}{}}
\newlabel{fig:skip-gram}{{2.4b}{9}{Diagrama de la arquitectura del modelo Skip-Gram. \relax }{figure.caption.15}{}}
\newlabel{sub@fig:skip-gram}{{b}{9}{Diagrama de la arquitectura del modelo Skip-Gram. \relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Modelos propuestos por Mikolov para representación y procesamiento de palabras.\relax }}{9}{figure.caption.15}\protected@file@percent }
\newlabel{fig:mikolovmodels}{{2.4}{9}{Modelos propuestos por Mikolov para representación y procesamiento de palabras.\relax }{figure.caption.15}{}}
\newlabel{def:one-hot}{{2}{9}{Codificación One-Hot}{defi.2}{}}
\citation{goldberg2014word2vec}
\citation{bellegarda2016state}
\citation{goldberg2014word2vec}
\citation{pennington2014glove}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Modelo GloVe}{10}{subsection.2.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Representación del embebido de palabras bajo $\mathbf  {P}$ de dimensión dos. Gráfico de elaboración propia.\relax }}{11}{figure.caption.16}\protected@file@percent }
\newlabel{fig:word_embedding}{{2.5}{11}{Representación del embebido de palabras bajo $\mathbf {P}$ de dimensión dos. Gráfico de elaboración propia.\relax }{figure.caption.16}{}}
\newlabel{eq:glove}{{2.4}{11}{Modelo GloVe}{equation.2.2.4}{}}
\newlabel{eq:glovels}{{2.5}{11}{Modelo GloVe}{equation.2.2.5}{}}
\citation{shi2014linking}
\citation{chomsky2004estructuras}
\citation{clark2011text}
\citation{billal2016efficient}
\citation{hassan2013social}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Pre-procesamiento}{12}{section.2.3}\protected@file@percent }
\newlabel{sec:sec23}{{2.3}{12}{Pre-procesamiento}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Normalización}{12}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Problemas por énfasis y expresiones regulares}{12}{subsubsection*.17}\protected@file@percent }
\citation{goyvaerts2012regular}
\citation{cohen1991introduction}
\@writefile{toc}{\contentsline {paragraph}{Nivel de carácter.}{13}{paragraph*.18}\protected@file@percent }
\citation{lopez2014mastering}
\citation{lopez2014mastering}
\citation{lopez2014mastering}
\citation{lopez2014mastering}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Expresiones predefinidas en Python. Tabla de elaboración propia con base en lo expuesto en la obra de \cite  {lopez2014mastering}. \relax }}{14}{table.caption.19}\protected@file@percent }
\newlabel{tb:caracterclassre}{{2.1}{14}{Expresiones predefinidas en Python. Tabla de elaboración propia con base en lo expuesto en la obra de \cite {lopez2014mastering}. \relax }{table.caption.19}{}}
\@writefile{toc}{\contentsline {paragraph}{Nivel de expresión regular.}{14}{paragraph*.20}\protected@file@percent }
\citation{hladek2020survey}
\citation{hladek2020survey}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Delimitadores de expresiones regulares. Tabla de elaboración propia con base en lo lo expuesto en la obra de \cite  {lopez2014mastering}. \relax }}{15}{table.caption.21}\protected@file@percent }
\newlabel{tb:redelims}{{2.2}{15}{Delimitadores de expresiones regulares. Tabla de elaboración propia con base en lo lo expuesto en la obra de \cite {lopez2014mastering}. \relax }{table.caption.21}{}}
\@writefile{toc}{\contentsline {paragraph}{Generalidades.}{15}{paragraph*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Corrección automática de errores}{15}{subsubsection*.23}\protected@file@percent }
\citation{damerau1964technique}
\citation{waggener1995pulse}
\citation{levenshtein1966binary}
\@writefile{toc}{\contentsline {paragraph}{Modelo del error.}{16}{paragraph*.24}\protected@file@percent }
\newlabel{eq:hammingstring}{{2.7}{16}{Distancia de Hamming}{equation.2.3.7}{}}
\citation{flouri2015longest}
\citation{hladek2020survey}
\citation{hladek2020survey}
\citation{melero2016selection}
\@writefile{toc}{\contentsline {paragraph}{Modelo de de contexto.}{17}{paragraph*.25}\protected@file@percent }
\citation{small2011hashtag}
\newlabel{eq:melero1}{{2.8}{18}{Modelo de de contexto}{equation.2.3.8}{}}
\newlabel{eq:melero2}{{2.10}{18}{Modelo de de contexto}{equation.2.3.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Segmentación}{18}{subsection.2.3.2}\protected@file@percent }
\citation{norvig209natural}
\citation{cano2019segmentacion}
\newlabel{eq:norvig}{{2.11}{19}{Segmentación}{equation.2.3.11}{}}
\newlabel{eq:norvigunknown}{{2.12}{19}{Segmentación}{equation.2.3.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Reconocimiento de Entidades Nombradas}{19}{subsection.2.3.3}\protected@file@percent }
\citation{nouvel2016named}
\citation{ross2014introduction}
\citation{nouvel2016named}
\newlabel{eq:ner1}{{2.13}{20}{Reconocimiento de Entidades Nombradas}{equation.2.3.13}{}}
\newlabel{eq:hmmsolve}{{2.14}{20}{Reconocimiento de Entidades Nombradas}{equation.2.3.14}{}}
\citation{sutton2019introduction}
\newlabel{fig:hmm}{{2.6a}{21}{Diagrama de un Modelo de Markov Oculto. Gráfico de elaboración propia. \relax }{figure.caption.26}{}}
\newlabel{sub@fig:hmm}{{a}{21}{Diagrama de un Modelo de Markov Oculto. Gráfico de elaboración propia. \relax }{figure.caption.26}{}}
\newlabel{fig:hmmner}{{2.6b}{21}{Diagrama de un Modelo de Markov Oculto en el contexto del NER aplicado al segundo tweet de ejemplo en este apartado. En este caso, la etiqueta 'O' significa 'Otro' para denotar que no es una entidad nombrada. Gráfico de elaboración propia. \relax }{figure.caption.26}{}}
\newlabel{sub@fig:hmmner}{{b}{21}{Diagrama de un Modelo de Markov Oculto en el contexto del NER aplicado al segundo tweet de ejemplo en este apartado. En este caso, la etiqueta 'O' significa 'Otro' para denotar que no es una entidad nombrada. Gráfico de elaboración propia. \relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Diagrama de Modelo Oculto de Markov, en verde se muestra la cadena de Markov oculta y en azul las señales observables.\relax }}{21}{figure.caption.26}\protected@file@percent }
\newlabel{fig:hmms}{{2.6}{21}{Diagrama de Modelo Oculto de Markov, en verde se muestra la cadena de Markov oculta y en azul las señales observables.\relax }{figure.caption.26}{}}
\newlabel{eq:hmmsolveequiv1}{{2.15}{21}{Reconocimiento de Entidades Nombradas}{equation.2.3.15}{}}
\newlabel{eq:hmmsolveequiv2}{{2.16}{21}{Reconocimiento de Entidades Nombradas}{equation.2.3.16}{}}
\newlabel{eq:hmmsolveequiv3}{{2.17}{21}{Reconocimiento de Entidades Nombradas}{equation.2.3.17}{}}
\newlabel{eq:hmmsolveequiv4}{{2.18}{21}{Reconocimiento de Entidades Nombradas}{equation.2.3.18}{}}
\citation{mikolov2013efficient}
\citation{rong2014word2vec}
\citation{goldberg2014word2vec}
\citation{goldberg2014word2vec}
\newlabel{def:caccl}{{7}{22}{Campo Aleatorio Condicional de Cadena Lineal}{defi.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Optimización}{22}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Estimación de los modelos CBOW y Skip-Gram}{22}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Muestreo negativo.}{22}{paragraph*.27}\protected@file@percent }
\newlabel{eq:losscbow}{{2.21}{22}{Muestreo negativo}{equation.2.4.21}{}}
\newlabel{eq:losscbow2}{{2.22}{22}{Muestreo negativo}{equation.2.4.22}{}}
\citation{morin2005hierarchical}
\citation{rojas1996backpropagation}
\@writefile{toc}{\contentsline {paragraph}{Optimización jerárquica.}{23}{paragraph*.28}\protected@file@percent }
\newlabel{eq:optijerar1}{{2.24}{23}{Optimización jerárquica}{equation.2.4.24}{}}
\@writefile{toc}{\contentsline {paragraph}{Algoritmo de propagación hacia atrás.}{23}{paragraph*.29}\protected@file@percent }
\citation{bellegarda2016state}
\citation{munkres2000topolgy}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Algoritmo de Viterbi}{24}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Algoritmos meta-heurísticos: Evolución diferencial}{24}{subsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Notas adicionales sobre el Procesamiento de Lenguaje Natural}{24}{section.2.5}\protected@file@percent }
\newlabel{sec:sec25}{{2.5}{24}{Notas adicionales sobre el Procesamiento de Lenguaje Natural}{section.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Sobre la similitud semántica}{24}{subsection.2.5.1}\protected@file@percent }
\newlabel{eq:hausdoff_original}{{2.28}{24}{Sobre la similitud semántica}{equation.2.5.28}{}}
\newlabel{eq:hausdoff_words}{{2.29}{24}{Sobre la similitud semántica}{equation.2.5.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Silabeo}{24}{subsection.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Sobre aplicaciones terminales}{24}{subsection.2.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Teoría lingüística adicional}{24}{section.2.6}\protected@file@percent }
\@setckpt{chapter2}{
\setcounter{page}{25}
\setcounter{equation}{29}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{17}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{6}
\setcounter{table}{2}
\setcounter{btxromaniannumeral}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{float@type}{16}
\setcounter{subfigure}{2}
\setcounter{subtable}{0}
\setcounter{parentequation}{0}
\setcounter{NAT@ctr}{0}
\setcounter{Item}{10}
\setcounter{Hfootnote}{17}
\setcounter{bookmark@seq@number}{27}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{lstnumber}{1}
\setcounter{defi}{7}
\setcounter{section@level}{0}
\setcounter{lstlisting}{0}
}
